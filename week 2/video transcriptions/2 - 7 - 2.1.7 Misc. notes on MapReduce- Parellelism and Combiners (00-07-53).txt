[NOISE]
Let's
talk about two extra topics in this video.
The first one I want to talk about
is Parallelism in MapReduce.
The way MapReduce abstraction is designed,
allows for easy parallelism of jobs.
Map functions in MapReduce,
by design, can run in parallel.
The way the framework
achieves that is easy.
It creates intermediate values
from different input data sets.
It feeds input data sets
into different maps.
They run totally
independent of each other,
the intermediate values
are collected by the framework.
So, different invocations of the map
function are completely independent of
each other and therefore it's very easy to
run them in parallel on a large cluster.
And that's where really MapReduce,
Hadoop for example,
gets its advantage in processing big data.
You can run it on clusters
of 10,000 machines.
The MapReduce abstraction guarantees that
all the invocations of map function
are completely independent.
Now, what about reduce function?
Reduce functions also can run in parallel.
Remembe,r just to point out,
you first have to wait until all of
the map functions are completed before you
can start any of the reduce functions.
Okay, so we know that.
But, once all of the maps are finished and
you start running invocations of
reduced functions across the cluster,
each reduce function can run independent
of any other reduce function.
And the reason for that is the framework
separates the intermediate key value
pairs, groups them by key,
and therefore different in
vocations of the reduce
function can completely run independent
of each other and in parallel.
All of these values
are processed independently.
Therefore, there's no requirement
to have communication
between different maps or
different reduces, so that works.
One bottleneck, of course, as I just
mentioned is that reduce functions have to
start after all of the maps are done.
There has been some research work
towards solving that as well, probably
not to the very general solution but
there's been lots of papers about that.
Now let's talk about the other issue here.
An interesting optimization in
a MapReduce framework is called Combiner.
It's a very common scenario when
the reduce function, well,
before the reduce function,
a map task will produce many
pairs of key values, right?
So the form could be any key on any value
and as we talked about in previous videos.
It's really up to the programmer to
decide what keys and what values to use.
But now let's imagine this.
Let's say you are running in
one map function, all right?
And you're working on
an input piece of data.
And your program just decides to create
different values with the same key.
So in this case, for example,
we have key being the same in
two different key value pairs.
For example,
you can imagine the word count example
we talked about,
popular words in a sentence.
You will definitely have more than
one key value pair with the same key.
For example, the word T-H-E,
the, or a, an.
These will be very common,
more than once repeated in a sentence and
therefore, your map function will produce
more key value pairs with the same key.
So, it would save network time
in these scenarios if you could
pre-aggregate the key value pairs
by key at the mapper, right?
So before letting the framework
start shuffling data,
shuffling key value pairs throughout the
network, trying to group all the key value
pairs with the same key in one machine,
you can say
at the source machine where the map
function is running, I can run
a combined function that looks very, very
similar to our reduced function typically.
But works on a partial set of data.
Not all of the data that the reduced
needs but a partial set of data and
creates a value.
Now, you can imagine this
cannot happen all the time.
When can this happen?
When can you work on a partial input data
set instead of the whole input data set
and do like a combine,
and another combine, and
then later on combine them again,
before sending all of them to a reduce.
You can only do that if your function
is commutative and associative.
So let's look at an example of
an associative and commutative function.
Addition is a good example.
If you have the addition operator and
you have A plus B,
commutativity says A + B = B + A,
which is the case,
and the associativity says
if you have A + (B + C),
that's equal to (A + B) + C.
Okay, so let's assume you have
these properties in your function.
You want to add numbers for example.
If that's what you need to do in the
reduce function, you can use combiners, it
saves a lot of network overhead, network
bandwidth, and makes your function faster.
And really, it's not that surprising to
see this sort of parallelism in MapReduce.
As we talked about
MapReduce really comes from
functional programming style of thinking,
right?
It's very common in practice to use
maps and reduces functional programming,
paradigms have been doing that for
a long time and its very much applicable
to big data sets because they
provide nice piles of semantics.
They really,
what they do is provide a nice
implementation of the scatter
gather data flow.
And that's very commonly
used in a lot of algorithms.
So, this works really fine in practice.
[MUSIC]

